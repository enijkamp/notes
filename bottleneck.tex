\documentclass[11pt]{article}

\usepackage[letterpaper, margin=1.6in]{geometry}
\usepackage{graphicx}
\usepackage{url}
\usepackage{epstopdf}
\usepackage{amssymb}
\usepackage{amsmath}

\usepackage{xcolor}
\newcommand\myworries[1]{\textcolor{red}{(#1)}}

% refs:
% (1) variational    https://arxiv.org/abs/1612.00410
% (2) black-box    https://arxiv.org/abs/1703.00810
% (3) youtube       https://www.youtube.com/watch?v=bLqJHjXihK8
% (4) original         http://www.cs.huji.ac.il/labs/learning/Papers/allerton.pdf

\begin{document}
	
\begin{center}
{\LARGE On the Information Bottleneck} 
\end{center}

\begin{abstract}
The Information Bottleneck (IB) formalizes the notion of a ``good'' representation in terms of the fundamental tradeoff between having a concise representation and one with good predictive power. It was introduced by Naftali Tishby et al. in 1999 and appears to be fundamental to a deep understanding of representations. We draw connections to (1) minimal sufficient statistics, (2) the formulation of variational auto-encoders, and (3) the topology of deep neural networks.
\end{abstract}

\section{Mutual Information}

% https://arxiv.org/abs/1703.00810
% https://en.wikipedia.org/wiki/Mutual_information

\section{Information Bottleneck}

% https://arxiv.org/pdf/1612.00410.pdf

Let random variable $X$ denote an input source, $Z$ a compressed representation, and $Y$ observed output. We assume a Markov chain $Y \leftrightarrow X \leftrightarrow Z$ \myworries{directed?}. That is, $Z$ cannot directly depend on $Y$. Then, the joint distribution $p(X,Y,Z)$ factorizes as
\begin{equation}
p(X, Y, Z) = p(Z\vert X,Y)p(Y\vert X)p(X) = p(Z\vert X)p(Y\vert X)p(X).
\end{equation}
where we assume $p(Z\vert X,Y) = p(Z\vert X)$.
Our goal is to learn an encoding $Z$ that is maximally informative about our target $Y$. As a measure we use the mutual information $I(Z,Y) \geq 0$ between our encoding $Z$ and output $X$
\begin{equation}
I(Z,Y) = \iint p(z,y) \log \frac{p(z,y)}{p(z)p(y)} dy\, dz = \iint p(y,z) \log \frac{p(y\vert z)}{p(y)}
\label{eq:I_Z_Y}
\end{equation}
where  $p(y\vert z)$ is fully defined by stochastic encoder $p(Z\vert X)$ and Markov chain as
\begin{equation}
p(y\vert z) = \int p(x,y\vert z) dx = \int p(y \vert x) p(x \vert z) dx = \int \frac{p(y \vert x)p(z \vert x) p(x)}{p(z)}dx.
\label{eq:p_y_z}
\end{equation}
\myworries{refine with \url{https://arxiv.org/abs/1703.00810}}
Recall, the mutual information $I(X,Y) = I(Y,X)$ quantifies the ``amount of information'' obtained about one random variable $X$, through the other random variable $Y$. It measures the inherent dependence expressed in the joint distribution of $X$ and $Y$ relative to the joint distribution of $X$ and $Y$ under the assumption of independence. If $X \perp Y$, then $p(x,y) = p(x)p(y)$, and therefore:
\begin{equation}
X \perp Y \Leftrightarrow \log\frac{p(x,y)}{p(x)p(y)} = \log 1 \Leftrightarrow I(X,Y) = 0.
\end{equation}
The concept is intricately linked to that of entropy of a random variable, a fundamental notion that defined ``amount of information'' held in a random variable:
\begin{equation}
I(X,Y) = H(X) - H(X \vert Y) = H(X) - H(X \vert Z) = H(X) + H(Y) - H(X,Y)
\end{equation}
where $H(\cdot)$ denotes marginal and $H(\cdot,\cdot)$ joint entropy.

If maximizing (\ref{eq:I_Z_Y}) was our only objective, then the trivial identity encoding $(Z = X)$ would always ensure a maximal informative representation. Instead, we would like to find the maximally informative representation subject to a constraint on it's complexity. Naturally, we constrain the mutual information between our encoding $Z$ and the input data $X$ such that $I(X,Z) \leq I_c$ where $I_c$ denotes the information constraint. This suggests our objective:
\begin{equation}
\underset{P(Z\vert X)}{\min} I(Z,Y) \quad\text{s.t.}\quad I(X,Z) \leq I_c.
\end{equation}
\myworries{$P(Z\vert X)$ correct?}
\myworries{doesn't match with \url{https://en.wikipedia.org/wiki/Information_bottleneck_method}, see comment 3 page 1}
Equivalently, we introduce a Lagrange multiplier $\beta$ and write the objective as:
\begin{equation}
R(\theta) = I(Z,Y) - \beta I(Z,X).
\label{eq:IB}
\end{equation}
\myworries{$\theta$?}
Here, our goal is to learn an encoding $Z$ that is maximally expressive about $Y$ while being maximally compressive about $X$. Then, $\beta\geq 0$ controls the tradeoff between informativeness and compression where large $\beta$ corresponds to highly compressed representations. This approach is known as the Information Bottleneck (IB). Intuitively, the first term in (\ref{eq:IB}) encourages $Z$ to be ``predictive'' of $Y$; the second term encourages $Z$ to ``forget'' $X$. Essentially, it forces $Z$ to act like a minimal sufficient statistic of $X$ for predicting $Y$.

The IB is appealing, since it defines a ``good'' representation in terms of the fundamental tradeoff between having a concise representation and one with good predictive power. The main drawback is that computing the mutual information is, in general, computationally challenging since (\ref{eq:p_y_z}) is intractable.

\section{Minimal Sufficient Statistics}

\section{Variational Formulation}

% https://arxiv.org/pdf/1612.00410.pdf

\myworries{$\beta$-VAE here}

\section{Information Plane}

% https://arxiv.org/abs/1703.00810
% https://www.youtube.com/watch?v=bLqJHjXihK8



\myworries{youtube deep-NN here}
\myworries{generalization bound here}

\bibliographystyle{plain}
\bibliography{\jobname} 

\end{document}
