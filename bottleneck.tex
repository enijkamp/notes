\documentclass[11pt]{article}

\usepackage[letterpaper, margin=1.6in]{geometry}
\usepackage{graphicx}
\usepackage{url}
\usepackage{epstopdf}
\usepackage{amssymb}
\usepackage{amsmath}

\usepackage{xcolor}
\newcommand\myworries[1]{\textcolor{red}{(#1)}}

% refs:
% (1) variational    https://arxiv.org/abs/1612.00410
% (2) black-box    https://arxiv.org/abs/1703.00810
% (3) youtube       https://www.youtube.com/watch?v=bLqJHjXihK8
% (4) original         http://www.cs.huji.ac.il/labs/learning/Papers/allerton.pdf

\begin{document}
	
\begin{center}
{\LARGE On the Information Bottleneck} 
\end{center}

\begin{abstract}
The Information Bottleneck (IB) formalizes the notion of an information-theoretic ``optimal'' representation in terms of the fundamental tradeoff between having a concise representation and one with good predictive power. It was introduced by Naftali Tishby et al. in 1999 and appears to be fundamental to a deep understanding of representations. We draw connections to (1) minimal sufficient statistics, (2) the formulation of variational auto-encoders, and, (3) the topology of deep neural networks.
\end{abstract}

\section{Mutual Information}

% https://arxiv.org/abs/1703.00810
% https://en.wikipedia.org/wiki/Mutual_information

Given any two random variables, $X$ and $Y$, with joint distribution $p(x, y)$, their Mutual Information $I(X,Y) = I(Y,X) \geq 0$ is defined as:
\begin{align}
\begin{split}
I(X,Y)
&= D_{KL}[p(x,y)\rVert p(x)p(y)] \\[4pt]
&= \iint p(x,y) \log \frac{p(x,y)}{p(x)p(y)} dx\,dy \\
&= \iint p(x,y) \log \frac{p(x\vert y)}{p(x)} dx\,dy \\[4pt]
&= H(X) - H(X\vert Y) 
\end{split}
\end{align}
where $D_{KL}[p\rVert q]$ denotes the Kullback-Leibler divergence of distributions $p$ and $q$, and $H(X)$ and $H(X\vert Y)$ are the entropy and conditional entropy of $X$ and $Y$, respectively. Note, if $X \perp Y$, then $p(x,y) = p(x)p(y)$, and therefore:
\begin{equation}
X \perp Y \; \Leftrightarrow \; \log\frac{p(x,y)}{p(x)p(y)} = \log 1 \; \Leftrightarrow \; I(X,Y) = 0.
\end{equation}
The concept is intricately linked to that of entropy of a random variable, a fundamental notion that defined ``amount of information'' held in a random variable:
\begin{equation}
I(X,Y) = H(X) - H(X \vert Y) = H(X) - H(X \vert Z) = H(X) + H(Y) - H(X,Y).
\end{equation}
The mutual information $I(X,Y)$ quantifies the ``amount of information'', the average number of relevant bits, obtained about one random variable $X$, through the other random variable $Y$. It measures the inherent dependence expressed in the joint distribution of $X$ and $Y$ relative to the joint distribution of $X$ and $Y$ under the assumption of independence. Define $X$ as some input variable and $Y$ as the label. Then, an optimal learning problem can be cast as the construction of an \textit{optimal encoder} of that relevant information via an efficient representation, a minimal sufficient statistic of $X$ with respect to $Y$, if such can be found. A minimal sufficient statistic can enable the \textit{decoding} of the relevant information with the smallest number of binary questions (on average), i.e. an \textit{optimal code}.
Two properties of the mutual information are fundamental in our context. First, the invariance to invertible transformations
\begin{equation}
I(X,Y) = I(\psi(X), \phi(Y))
\end{equation}
for any invertible functions $\psi(\cdot)$ and $\phi(\cdot)$. Second, the ``Data Processing Inequality'' (DPI): for any 3 random variables which form a Markov chain $X\rightarrow Y \rightarrow Z$ it holds
\begin{equation}
I(X,Y) \geq I(X,Z).
\label{eq:dpi}
\end{equation}

\section{Information Bottleneck}

% https://arxiv.org/pdf/1612.00410.pdf

\myworries{refine with \url{https://arxiv.org/abs/1703.00810}}

Let random variable $X$ denote an input source, $Z$ a compressed representation, and $Y$ observed output. We assume a Markov chain $X\rightarrow Y \rightarrow Z$. That is, $Z$ cannot directly depend on $Y$. Then, the joint distribution $p(X,Y,Z)$ factorizes as
\begin{equation}
p(X, Y, Z) = p(Z\vert X,Y)p(Y\vert X)p(X) = p(Z\vert X)p(Y\vert X)p(X).
\end{equation}
where we assume $p(Z\vert X,Y) = p(Z\vert X)$.
Our goal is to learn an encoding $Z$ that is maximally informative about our target $Y$. As a measure we use the mutual information $I(Z,Y) \geq 0$ between our encoding $Z$ and output $X$
\begin{equation}
I(Z,Y) = \iint p(z,y) \log \frac{p(z,y)}{p(z)p(y)} dy\, dz = \iint p(y,z) \log \frac{p(y\vert z)}{p(y)}
\label{eq:I_Z_Y}
\end{equation}
where  $p(y\vert z)$ is fully defined by stochastic encoder $p(Z\vert X)$ and Markov chain as
\begin{equation}
p(y\vert z) = \int p(x,y\vert z) dx = \int p(y \vert x) p(x \vert z) dx = \int \frac{p(y \vert x)p(z \vert x) p(x)}{p(z)}dx.
\label{eq:p_y_z}
\end{equation}
If maximizing (\ref{eq:I_Z_Y}) was our only objective, then the trivial identity encoding ${(Z = X)}$ would always ensure a maximal informative representation. Instead, we would like to find the maximally informative representation subject to a constraint on it's complexity. Naturally, we constrain the mutual information between our encoding $Z$ and the input data $X$ such that $I(X,Z) \leq I_c$ where $I_c$ denotes the information constraint. This suggests our objective:
\begin{equation}
\underset{P(Z\vert X)}{\min} I(Z,Y) \quad\text{s.t.}\quad I(X,Z) \leq I_c.
\end{equation}
\myworries{$P(Z\vert X)$ correct?}
\myworries{doesn't match with \url{https://en.wikipedia.org/wiki/Information_bottleneck_method}, see comment 3 page 1}
Equivalently, we introduce a Lagrange multiplier $\beta$ and write the objective as:
\begin{equation}
R(\theta) = I(Z,Y) - \beta I(Z,X).
\label{eq:IB}
\end{equation}
\myworries{$\theta$?}
Here, our goal is to learn an encoding $Z$ that is maximally expressive about $Y$ while being maximally compressive about $X$. Then, $\beta\geq 0$ controls the tradeoff between informativeness and compression where large $\beta$ corresponds to highly compressed representations. This approach is known as the Information Bottleneck (IB). Intuitively, the first term in (\ref{eq:IB}) encourages $Z$ to be ``predictive'' of $Y$; the second term encourages $Z$ to ``forget'' $X$. Essentially, it forces $Z$ to act like a minimal sufficient statistic of $X$ for predicting $Y$.

The IB is appealing, since it defines a ``good'' representation in terms of the fundamental tradeoff between having a concise representation and one with good predictive power. The main drawback is that computing the mutual information is, in general, computationally challenging since (\ref{eq:p_y_z}) is intractable.

\section{Minimal Sufficient Statistics}

What characterizes the optimal representation of $X$ with respect to $Y$? The classical notion of minimal sufficient statistics provides good candidates for optimal representations. In our setting, sufficient statistics $S(X)$ are a partitioning on $X$, that captures all the information that $X$ has on $Y$. That is, $I(S(X), Y) = I(X,Y)$.

Minimal sufficient statistics, $T(X)$, are the simplest sufficient statistics and induce the coarsest sufficient partition on $X$. Formally, they are functions of any other sufficient statistic. We can formulate this by a Markov chain:
\begin{equation}
Y\rightarrow X\rightarrow S(X) \rightarrow T(X),
\end{equation}
which holds for any minimal sufficient statistic $T(X)$ with any other sufficient statistic $S(X)$. Using the DPI in (\ref{eq:dpi}), we cast this into an optimization problem:
\begin{equation}
T(X) = \underset{\{S(X):I(S(X,Y))=I(X,Y)\}}{\arg\min} I(S(X), X).
\end{equation}
Since exact minimal sufficient statistics only exist for distributions of exponential families, Tishby relaxed this optimization problem by first, allowing the map to be stochastic, defined as an encoder $P(T\vert X)$, and second, by allowing the map to capute \textit{as much as possible} of $I(X,Y)$, not necessarily all of it.
This leads to the \textit{Information Bottleneck} tradeoff, which provides a computational framework for finding approximate minimal sufficient statistics, or, the optimal tradeoff between compression of $X$ and prediction of $Y$. In this sense, efficient representations are approximate minimal sufficient statistics.
Define $t\in T$ as a compressed representation of $x\in X$, then the mapping $p(t\vert x)$ defines the representation of $x$. This Information Bottleneck tradeoff is formulated by the following optimization problem, carried independently for the distributions $p(t\vert x), p(t), p(y\vert t)$, with Markov chain $Y\rightarrow X \rightarrow T$,
\begin{equation}
\underset{p(t\vert x), p(t), p(y\vert t)}{\min} \{ I(X,T)-\beta I(T,Y) \}.
\end{equation}
The Lagrange multipler $\beta$ determines the level of relevant information $I(T,Y)$ captured by the representation $T$, which is directly related to the error in the label prediction from this representation.

\section{Variational Formulation}

% https://arxiv.org/pdf/1612.00410.pdf

\myworries{$\beta$-VAE here}

\section{Information Plane}

% https://arxiv.org/abs/1703.00810
% https://www.youtube.com/watch?v=bLqJHjXihK8



\myworries{youtube deep-NN here}
\myworries{generalization bound here}

\bibliographystyle{plain}
\bibliography{\jobname} 

\end{document}
